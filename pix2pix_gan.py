# -*- coding: utf-8 -*-
"""pix2pix GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L--9MNz0xR09rFtEQwW-3eg2zJ8gACXu
"""

import matplotlib.pyplot as plt
import numpy as num
import os
import keras 
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')

path=r"/content/drive/MyDrive/train"

from keras.models import Model
from keras.models import Input
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Activation
from keras.layers import Concatenate
from keras.layers import Dropout
from keras.layers import BatchNormalization
from matplotlib import pyplot as plt
from tensorflow.keras.utils import plot_model
from keras.initializers import RandomNormal
from keras.layers import concatenate,MaxPooling2D

x_train=[]
y_train=[]
import cv2

#lets arrange the data and ground truths
# if ou look at the original data then ou might see the data and the ground truth are present together so we had to separate
# so upto 600 fro x_axis we have our data and from 600 to the rest we have our ground truth

for img in os.listdir(path):
  img_r=cv2.imread(os.path.join(path,img))
  

  x_train.append(cv2.resize(img_r[:,:600],(256,256)))
  y_train.append(cv2.resize(img_r[:,600:],(256,256)))

x_train=num.array(x_train)
y_train=num.array(y_train)

print(x_train.shape)
print(y_train.shape)

plt.imshow(y_train[0])

x_train=(x_train-127.5)/127.5
y_train=(y_train-127.5)/127.5

h=x_train.shape[1]
w=x_train.shape[2]
channel=x_train.shape[3]

print(h,w,channel)

# lets built our generator which is actually a u-net 
#as of for this project our generator is creating satellite map view (like google maps) from real satellite images

def build_unet(x,y,z):
    inputs=Input(shape=(x,y,z))
    init = RandomNormal(stddev=0.02)
    c1=Conv2D(64,(4,4),strides=(2,2),activation="relu",kernel_initializer=init,padding="same")(inputs)
    c1=LeakyReLU(alpha=0.2)(c1)

    c2 = Conv2D(128, (4,4),strides=(2,2), kernel_initializer=init, padding='same')(c1)
    c2=BatchNormalization()(c2,training=True)
    c2=LeakyReLU(alpha=0.2)(c2)

    c3 = Conv2D(256 ,(4,4), strides=(2,2) ,kernel_initializer=init, padding='same')(c2)
    c3=BatchNormalization()(c3,training=True)
    c3=LeakyReLU(alpha=0.2)(c3)

    c4 = Conv2D(512, (4,4), strides=(2,2), kernel_initializer=init, padding='same')(c3)
    c4=BatchNormalization()(c4,training=True)
    c4=LeakyReLU(alpha=0.2)(c4)

    c5 = Conv2D(512, (4,4),strides=(2,2),  kernel_initializer=init, padding='same')(c4)
    c5=BatchNormalization()(c5,training=True)
    c5=LeakyReLU(alpha=0.2)(c5)  

    c6 = Conv2D(512, (4,4), strides=(2,2),kernel_initializer=init, padding='same')(c5)
    c6=BatchNormalization()(c6,training=True)
    c6=LeakyReLU(alpha=0.2)(c6)  

    c7 = Conv2D(512, (4,4), strides=(2,2), kernel_initializer=init, padding='same')(c6)
    c7=BatchNormalization()(c7,training=True)
    c7=LeakyReLU(alpha=0.2)(c7)  

    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(c7)
    b = Activation('relu')(b)  

    d1 = Conv2DTranspose(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(b)
    d1=BatchNormalization()(d1,training=True)
    d1=Dropout(0.5)(d1,training=True)
    d1=Concatenate()([d1, c7])   
    d1=Activation("relu")(d1)

    d2 =Conv2DTranspose(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d1)
    d2=BatchNormalization()(d2,training=True)
    d2=Dropout(0.5)(d2,training=True)
    d2=Concatenate()([d2,c6])  
    d2=Activation("relu")(d2)

    d3 = Conv2DTranspose(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d2)
    d3=BatchNormalization()(d3,training=True)
    d3=Dropout(0.5)(d3,training=True)
    d3=Concatenate()([d3, c5])   
    d3=Activation("relu")(d3)

    d4 = Conv2DTranspose(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d3)
    d4=BatchNormalization()(d4,training=True)
    d4=Concatenate()([d4, c4])   
    d4=Activation("relu")(d4) 

    d5 = Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d4)
    d5=BatchNormalization()(d5,training=True)
    d5=Concatenate()([d5, c3])   
    d5=Activation("relu")(d5) 

    d6 = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d5)
    d6=BatchNormalization()(d6,training=True)
    d6=Concatenate()([d6, c2])   
    d6=Activation("relu")(d6)     

    d7 = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d6)
    d7=BatchNormalization()(d7,training=True)
    d7=Concatenate()([d7, c1])   
    d7=Activation("relu")(d7)    

    f = Conv2DTranspose(z, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7) 
    out_image = Activation('tanh')(f) 

    model = Model(inputs, out_image)
    return model

# in our disriminator we give our source image and target image as the input 
#and expect patches output predicting if its from real or fake

def discriminator(h,w,c):
    init = RandomNormal(stddev=0.02) #As described in the original paper
      
    
    in_src_image = Input(shape=(h,w,c))  
    
    in_target_image = Input(shape=(h,w,c))   
      
    
    merged = Concatenate()([in_src_image, in_target_image])
      
    
    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)
    d = LeakyReLU(alpha=0.2)(d)
    
    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
    d = BatchNormalization()(d)
    d = LeakyReLU(alpha=0.2)(d)
    
    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
    d = BatchNormalization()(d)
    d = LeakyReLU(alpha=0.2)(d)

    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)
    d = BatchNormalization()(d)
    d = LeakyReLU(alpha=0.2)(d)
    
    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)
    d = BatchNormalization()(d)
    d = LeakyReLU(alpha=0.2)(d)
    
    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)
    patch_out = Activation('sigmoid')(d)

    model = Model([in_src_image, in_target_image], patch_out)

      
    model.compile(loss='binary_crossentropy', optimizer="adam", loss_weights=[0.5])
    return model

#combining our generator and discriminator to create a GAN 
#here we add another loss in our thats from our generator (mean-absolute-error )
#we also specify the weightage of each loss

def GAN(gen,dis,h,w,c):
    for layer in dis.layers:
      if not isinstance(layer, BatchNormalization):
        layer.trainable = False         
    inp1=Input((256,256,3))

    out=gen(inp1)
    main_out=dis([inp1,out])
    model=Model(inp1,[main_out,out])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002,beta_1=0.5),loss=["binary_crossentropy","mae"],loss_weights=[1,100])
    return model

##Training our model at first we keep our discriminator non trainable

def train(gen,dis,comb,batch_size,epochs):
  half_batch=int(batch_size/2)
  batch_step=int(x_train.shape[0]/batch_size)
  for epoch in range(epochs):
    s=0
    for batch in range(batch_step):
      y_fake=gen.predict(x_train[s:s+half_batch])
      y_real=y_train[s:s+half_batch]
      x_real=x_train[s:s+half_batch]
      d_loss_real=dis.train_on_batch([y_real,x_real],num.ones((half_batch,16,16,1)))
      d_loss_fake=dis.train_on_batch([y_fake,x_real],num.zeros((half_batch,16,16,1)))
      fak_pred=num.ones((batch_size,16,16,1))

      gan_loss,_,_=comb.train_on_batch(x_train[s:s+batch_size],[fak_pred,y_train[s:s+batch_size]])
    print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (epoch, d_loss_real, d_loss_fake,gan_loss)   )

x_train.shape[0]/64

num.ones((64,1))[1:10]

gen=build_unet(256,256,3)

#gen.compile(optimizer="adam",loss="binary_crossentropy",run_eagerly=True)

gen.summary()

dis=discriminator(h,w,channel)

dis.summary()

model=GAN(gen,dis,256,256,3)

model.summary()

train(gen,dis,model,10,100)

y_f=gen.predict(x_train[:25])

plt.subplot(1,2,1)
plt.imshow(y_f[3])
plt.subplot(1,2,2)
plt.imshow(y_train[3])









